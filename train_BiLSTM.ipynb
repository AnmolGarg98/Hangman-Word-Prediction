{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/home/nidhi2023/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "    \n",
    "full_dictionary_location = \"words_250000_train.txt\"\n",
    "full_dictionary = build_dictionary(full_dictionary_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict_Dataset(Dataset):\n",
    "    def __init__(self, dictionary):\n",
    "        self.words = dictionary\n",
    "        # Create a mapping from each letter to its corresponding index (0-25)\n",
    "        self.alphabets = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.CHAR_TO_INDEX = {char: idx for idx, char in enumerate(self.alphabets)}\n",
    "\n",
    "        self.letter_weight = {}\n",
    "        for i in dictionary:\n",
    "            for l in self.alphabets:\n",
    "                if l in i:\n",
    "                    if self.letter_weight.get(l):\n",
    "                        self.letter_weight[l] += 1\n",
    "                    else:\n",
    "                        self.letter_weight[l] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "    \n",
    "    def cnt_to_guesses(self,char_set,cnt):\n",
    "        lst = list(char_set)\n",
    "        return list(np.random.choice(lst, cnt, p=[self.letter_weight[i] for i in lst]/np.sum([self.letter_weight[i] for i in lst]), replace=False))\n",
    "    \n",
    "    def one_hot_encode(self,char):\n",
    "        \"\"\"Convert a character to a one-hot vector.\"\"\"\n",
    "        vec = torch.zeros(28)\n",
    "        vec[self.CHAR_TO_INDEX[char]] = 1.0\n",
    "        return vec\n",
    "\n",
    "    def word_to_matrix(self,word, correct_guesses,wrong_guesses):\n",
    "        word = word.lower()  # Ensure the word is lowercase\n",
    "        matrix = torch.zeros(38, 28)  # Initialize a (38, 28) matrix with zeros 27,28 digits for depicting blanks and wrong vector respect.\n",
    "        # matrix[:len(word),26]=1  ## depicting word length in our sequence\n",
    "        # print(matrix)\n",
    "        for i, char in enumerate(word):\n",
    "            if char in correct_guesses:\n",
    "                # print(char)\n",
    "                matrix[i] = self.one_hot_encode(char)\n",
    "        # print('rssa')\n",
    "        for i, char in enumerate(wrong_guesses):\n",
    "            # print(char)\n",
    "            matrix[32+i] = self.one_hot_encode(char)  \n",
    "            matrix[32+i,27]=1\n",
    "        matrix[:len(word),26]=1  ## depicting word length in our sequence\n",
    "        return matrix\n",
    "\n",
    "    def multi_encode(self,set_char):\n",
    "        string = ''.join(set_char)\n",
    "        vec = torch.zeros(26)\n",
    "        for char in string:\n",
    "            vec[self.CHAR_TO_INDEX[char]] = 1.0\n",
    "        return vec\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        wrd = self.words[idx]\n",
    "        set_alpha = set(wrd)\n",
    "\n",
    "        if len(set_alpha)==1:\n",
    "            return self.word_to_matrix( wrd, '', '' ), self.multi_encode(set_alpha)\n",
    "        \n",
    "        if 'e' in set_alpha:\n",
    "            cnt_correct_guess = np.random.randint(len(set_alpha)-1) ## -1 for e, -1 for atleast one unguessed\n",
    "            cnt_incorrect_guess = np.random.randint(6) ## 0 to 5\n",
    "            correct_guesses = self.cnt_to_guesses(set_alpha-set('e'),cnt_correct_guess)\n",
    "            wrong_guesses = self.cnt_to_guesses(set(self.alphabets)-set_alpha,cnt_incorrect_guess)\n",
    "            return self.word_to_matrix( wrd, ''.join(correct_guesses)+'e', ''.join(wrong_guesses) ), self.multi_encode(set_alpha - set(''.join(correct_guesses)+'e'))\n",
    "        \n",
    "        elif 'a' in set_alpha:\n",
    "            cnt_correct_guess = np.random.randint(len(set_alpha)-1) ## -1 for a, -1 for atleast one unguessed\n",
    "            cnt_incorrect_guess = np.random.randint(5) ## 0 to 4 , one for 'e'\n",
    "            correct_guesses = self.cnt_to_guesses(set_alpha-set('a'),cnt_correct_guess)\n",
    "            wrong_guesses = self.cnt_to_guesses(set(self.alphabets)-set_alpha-set('e'),cnt_incorrect_guess)\n",
    "            return self.word_to_matrix( wrd, ''.join(correct_guesses)+'a', ''.join(wrong_guesses)+'e' ), self.multi_encode(set_alpha - set(''.join(correct_guesses)+'a'))\n",
    "        \n",
    "        elif 'i' in set_alpha:\n",
    "            cnt_correct_guess = np.random.randint(len(set_alpha)-1) ## -1 for i, -1 for atleast one unguessed\n",
    "            cnt_incorrect_guess = np.random.randint(4) ## 0 to 3 , two for 'e','a'\n",
    "            correct_guesses = self.cnt_to_guesses(set_alpha-set('i'),cnt_correct_guess)\n",
    "            wrong_guesses = self.cnt_to_guesses(set(self.alphabets)-set_alpha-set('ea'),cnt_incorrect_guess)\n",
    "            return self.word_to_matrix( wrd, ''.join(correct_guesses)+'i', ''.join(wrong_guesses)+'ea' ), self.multi_encode(set_alpha - set(''.join(correct_guesses)+'i'))\n",
    "        \n",
    "        elif 'o' in set_alpha:\n",
    "            cnt_correct_guess = np.random.randint(len(set_alpha)-1) ## -1 for o, -1 for atleast one unguessed\n",
    "            cnt_incorrect_guess = np.random.randint(3) ## 0 to 2 , three for 'e','a','i'\n",
    "            correct_guesses = self.cnt_to_guesses(set_alpha-set('o'),cnt_correct_guess)\n",
    "            wrong_guesses = self.cnt_to_guesses(set(self.alphabets)-set_alpha-set('eai'),cnt_incorrect_guess)\n",
    "            return self.word_to_matrix( wrd, ''.join(correct_guesses)+'o', ''.join(wrong_guesses)+'eai' ), self.multi_encode(set_alpha - set(''.join(correct_guesses)+'o'))\n",
    "        \n",
    "        else:\n",
    "            return self.word_to_matrix( wrd, '', 'eaio' ), self.multi_encode(set_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dict_Dataset(full_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length,device):\n",
    "\t\tsuper(BiLSTMClassifier, self).__init__()\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.output_size = output_size\n",
    "\t\tself.hidden_size = hidden_size\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.embedding_length = embedding_length\n",
    "\t\tself.device = device\n",
    "\t\t\n",
    "\t\tself.word_embeddings = nn.Linear(vocab_size, embedding_length)\n",
    "\t\t# self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) \n",
    "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size, bidirectional=True) # Our main hero for this tutorial\n",
    "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
    "\t\t\n",
    "\tdef forward(self, input_sentence, batch_size=None):\n",
    "\t\tinput = self.word_embeddings(input_sentence) \n",
    "\t\tinput = input.permute(1, 0, 2) \n",
    "\t\tif batch_size is None:\n",
    "\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).to(self.device)) \n",
    "\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).to(self.device)) \n",
    "\t\telse:\n",
    "\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).to(self.device))\n",
    "\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).to(self.device))\n",
    "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
    "\t\tfinal_output = self.label(final_hidden_state[-1]) \n",
    "\t\t\n",
    "\t\treturn final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:12<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 72.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:12<00:00, 49.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/150], Loss: 0.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 78.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:12<00:00, 49.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/150], Loss: 0.2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 72.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:15<00:00, 47.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/150], Loss: 0.2647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:08<00:00, 79.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:13<00:00, 48.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/150], Loss: 0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 71.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:11<00:00, 49.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/150], Loss: 0.2576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:08<00:00, 87.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:10<00:00, 50.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/150], Loss: 0.2555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:07<00:00, 93.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:16<00:00, 46.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/150], Loss: 0.2534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 74.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:16<00:00, 46.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/150], Loss: 0.2523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:07<00:00, 88.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [01:15<00:00, 47.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Loss: 0.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:09<00:00, 72.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [00:53<00:00, 66.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/150], Loss: 0.2502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:11<00:00, 62.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [00:33<00:00, 106.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/150], Loss: 0.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:05<00:00, 137.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [00:33<00:00, 105.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/150], Loss: 0.2483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:04<00:00, 143.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [00:33<00:00, 105.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/150], Loss: 0.2478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:05<00:00, 136.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3551/3551 [00:33<00:00, 104.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/150], Loss: 0.2473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 34/710 [00:00<00:08, 80.99it/s]"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 28  # Input size of each sequence element\n",
    "seq_len = 38    # Sequence length\n",
    "num_classes = 26  # Number of classes\n",
    "embed_size = 48  # Embedding size\n",
    "num_heads = 4    # Number of heads in multi-head attention\n",
    "hidden_dim = 196  # Hidden dimension size in the feedforward layer\n",
    "num_layers = 4  # Number of Transformer Encoder layers\n",
    "dropout = 0.1    # Dropout rate\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = Dict_Dataset(full_dictionary)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=6,drop_last=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True,num_workers=4,drop_last=True)\n",
    "device = 'cuda:2'\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BiLSTMClassifier(batch_size, num_classes, hidden_dim, input_dim, embed_size,device)\n",
    "# model.load_state_dict(torch.load('models/bilstm_best'))\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "max_epoch_loss = 99\n",
    "num_epochs = 150\n",
    "flag=0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm.tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        # print(outputs)\n",
    "        # stop\n",
    "        loss = criterion(outputs.cpu(), labels)\n",
    "        loss.backward()\n",
    "        clip_gradient(model, 1e-1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    if epoch_loss < max_epoch_loss:\n",
    "        flag = 1\n",
    "        max_epoch_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), f'models/bilstm_{epoch}')\n",
    "        torch.save(model.state_dict(), f'models/bilstm_best')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm.tqdm(val_dataloader):\n",
    "            outputs = model(inputs.to(device)).cpu()\n",
    "            # print(torch.argmax(outputs.data, 1))\n",
    "            # print(labels)\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # print(torch.sum(torch.Tensor([labels[ind,i] for ind,i in enumerate(predicted)])))\n",
    "            correct += torch.sum(torch.Tensor([labels[ind,i] for ind,i in enumerate(predicted)])).item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    if flag==1:\n",
    "        flag=0\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 710/710 [00:06<00:00, 116.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.64%\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 28  # Input size of each sequence element\n",
    "seq_len = 38    # Sequence length\n",
    "num_classes = 26  # Number of classes\n",
    "embed_size = 48  # Embedding size\n",
    "num_heads = 4    # Number of heads in multi-head attention\n",
    "hidden_dim = 196  # Hidden dimension size in the feedforward layer\n",
    "num_layers = 4  # Number of Transformer Encoder layers\n",
    "dropout = 0.1    # Dropout rate\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = Dict_Dataset(full_dictionary)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [0.8, 0.2])\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,num_workers=6,drop_last=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=True,num_workers=4,drop_last=True)\n",
    "device = 'cuda:2'\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BiLSTMClassifier(batch_size, num_classes, hidden_dim, input_dim, embed_size,device)\n",
    "model.load_state_dict(torch.load('models/bilstm_best'))\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "max_epoch_loss = 99\n",
    "num_epochs = 150\n",
    "flag=0\n",
    "for epoch in range(1):\n",
    "    # model.train()\n",
    "    # running_loss = 0.0\n",
    "    # for inputs, labels in tqdm.tqdm(train_dataloader):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = model(inputs.to(device))\n",
    "    #     # print(outputs)\n",
    "    #     # stop\n",
    "    #     loss = criterion(outputs.cpu(), labels)\n",
    "    #     loss.backward()\n",
    "    #     clip_gradient(model, 1e-1)\n",
    "    #     optimizer.step()\n",
    "        \n",
    "    #     running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # epoch_loss = running_loss / len(dataset)\n",
    "    # if epoch_loss < max_epoch_loss:\n",
    "    #     flag = 1\n",
    "    #     max_epoch_loss = epoch_loss\n",
    "    #     torch.save(model.state_dict(), f'models/bilstm_{epoch}')\n",
    "    #     torch.save(model.state_dict(), f'models/bilstm_best')\n",
    "    #     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm.tqdm(val_dataloader):\n",
    "            outputs = model(inputs.to(device)).cpu()\n",
    "            # print(torch.argmax(outputs.data, 1))\n",
    "            # print(labels)\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # print(torch.sum(torch.Tensor([labels[ind,i] for ind,i in enumerate(predicted)])))\n",
    "            correct += torch.sum(torch.Tensor([labels[ind,i] for ind,i in enumerate(predicted)])).item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
